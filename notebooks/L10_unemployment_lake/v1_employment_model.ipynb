{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Employment-Unemployment Model\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Goal**:\n",
    "\n",
    "Estimate a Markov chain using employment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steps to \"understanding the world around us\" (a short digression)\n",
    "\n",
    "\n",
    "Our friend Jim Savage has thought about what a \"modern statistical workflow\" entails. We summarize some steps he has proposed for understanding the world around us:\n",
    "\n",
    "> 1. Prepare and visualize your data.\n",
    "> 2. Create a generative model for the data...\n",
    ">   - A first model should be as simple as possible.\n",
    "> 3. Simulate some artificial data from your model given some assumed parameters that you \"pick out of a hat\" ($\\theta$)\n",
    "> 4. Use the artificial data to estimate the model parameters ($\\hat{\\theta})$\n",
    "> 5. Check that you recovered a good approximation of the \"known unknowns\" (aka, $\\theta \\approx \\hat{\\theta}$)\n",
    ">   - Possibly repeat 3-5 with different estimators and true parameters ($\\theta$), to get an understanding of how well the fitting procedure works\n",
    "> 6. Fit the model to your real data, check the fit\n",
    "> 7. Argue about the results with your friends and colleagues\n",
    "> 8. Go back to 2. with a slightly richer model. Repeat.\n",
    "> 9. Think carefully about what decisions will be made from the analysis, encode a loss function, and perform statistical decision analysis... Note that in many cases, we will do step 9 before steps 1-8!\n",
    "\n",
    "Later this semester, we will talk formally about what it means to \"fit\" your model (and the work that it entails), but, for now, we find it sufficient to say that it's a process to ensure that the probability distribution over outcomes generated by your model lines up with the data (aka, finding the right model parameters).\n",
    "\n",
    "We'll do a version of steps 1-6 to help us improve our understanding of the labor data that we previously saw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Our plan\n",
    "\n",
    "1. Prepare and visualize our data.\n",
    "2. Develop a generative model of employment and unemployment\n",
    "3. Simulate data from our generative model for given parameters\n",
    "4. Fit our model to the simulated data\n",
    "5. Explore different ways that we might have chosen to fit the data\n",
    "6. Fit the model with the BLS data\n",
    "7. Examine what our model implies for the effects of COVID on employment/unemployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Prepare and visualize our data\n",
    "\n",
    "We have done this in earlier lectures and will not repeat the work here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Create a generative model\n",
    "\n",
    "**A simple model of employment**\n",
    "\n",
    "In the vein of, \"the first model created should be as simple as possible\", we use the employment model that we studied earlier.\n",
    "\n",
    "Consider a single individual that transitions between employment and unemployment\n",
    "\n",
    "* When unemployed, they find a new job with probability $\\alpha$\n",
    "* When employed, they lose their job with probability $\\beta$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "![ModelFlowchart](model_diagram.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Simulate data from our generative model\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We will break simulating data from the model into two steps:\n",
    "\n",
    "1. Given today's state and the transition probabilities, draw from tomorrow's state\n",
    "2. Given an initial state and transition probabilities, simulate an entire history of employment/unemployment using the one-step transition kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simulate the one-step employment transition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def next_state(s_t, alpha, beta):\n",
    "    \"\"\"\n",
    "    Transitions from employment/unemployment in period t to\n",
    "    employment/unemployment in period t+1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s_t : int\n",
    "        The individual's current state... s_t = 0 maps to\n",
    "        unemployed and s_t = 1 maps to employed\n",
    "    alpha : float\n",
    "        The probability that an individual goes from\n",
    "        unemployed to employed\n",
    "    beta : float\n",
    "        The probability that an individual goes from\n",
    "        employed to unemployed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    s_tp1 : int\n",
    "        The individual's employment state in `t+1`\n",
    "    \"\"\"\n",
    "    # Draw a random number\n",
    "    u_t = np.random.rand()\n",
    "\n",
    "    # Let 0 be unemployed... If unemployed and draws\n",
    "    # a value less than lambda then becomes employed\n",
    "    if (s_t == 0) and (u_t < alpha):\n",
    "        s_tp1 = 1\n",
    "    # Let 1 be employed... If employed and draws a\n",
    "    # value less than beta then becomes unemployed\n",
    "    elif (s_t == 1) and (u_t < beta):\n",
    "        s_tp1 = 0\n",
    "    # Otherwise, he keeps the same state as he had\n",
    "    # at period t\n",
    "    else:\n",
    "        s_tp1 = s_t\n",
    "\n",
    "    return s_tp1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Notice how this function incorporates the Markov property that our model assumes.\n",
    "\n",
    "The Markov property says $\\text{Probability}(s_{t+1} | s_{t}) = \\text{Probability}(s_{t+1} | s_{t}, s_{t-1}, \\dots, s_0)$.\n",
    "\n",
    "This means that, other than the transition probabilities, we only need to know today's state and not the entire history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Testing our function**\n",
    "\n",
    "It's always a good idea to write some simple test cases for functions that we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "next_state(0, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Should never become employed from unemployment\n",
    "# if alpha is 0\n",
    "next_state(0, 0.0, 0.5) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Should never become unemployed from employment\n",
    "# if beta is 0\n",
    "next_state(1, 0.5, 0.0) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Should always transition to employment from unemployment\n",
    "# when alpha is 1\n",
    "next_state(0, 1.0, 0.5) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Should always transition to unemployment from employment\n",
    "# when beta is 1\n",
    "next_state(1, 0.5, 1.0) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simulate entire history**\n",
    "\n",
    "**Note**: Later we will allow $\\alpha$ and $\\beta$ to change over time, so while we want you think of them as constant for now, we will write code that allows for them to fluctate period-by-period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_employment_history(alpha, beta, s_0):\n",
    "    \"\"\"\n",
    "    Simulates the history of employment/unemployment. It\n",
    "    will simulate as many periods as elements in `alpha`\n",
    "    and `beta`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : np.array(float, ndim=1)\n",
    "        The probability that an individual goes from\n",
    "        unemployed to employed\n",
    "    beta : np.array(float, ndim=1)\n",
    "        The probability that an individual goes from\n",
    "        employed to unemployed\n",
    "    s_0 : int\n",
    "        The initial state of unemployment/employment, which\n",
    "        should take value of 0 (unemployed) or 1 (employed)\n",
    "    \"\"\"\n",
    "    # Create array to hold the values of our simulation\n",
    "    assert(len(alpha) == len(beta))\n",
    "    T = len(alpha)\n",
    "    s_hist = np.zeros(T+1, dtype=int)\n",
    "\n",
    "    s_hist[0] = s_0\n",
    "    for t in range(T):\n",
    "        # Step one period into the future\n",
    "        s_0 = next_state(s_0, alpha[t], beta[t])  # Notice alpha[t] and beta[t]\n",
    "        s_hist[t+1] = s_0\n",
    "\n",
    "    return s_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Check output of the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alpha = np.ones(50)*0.25\n",
    "beta = np.ones(50)*0.025\n",
    "\n",
    "simulate_employment_history(alpha, beta, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Fit your model to our artificial data\n",
    "\n",
    "There are lots of procedures that one could use to infer parameters values from data. Here we'll just count relative frequencies of transitions.\n",
    "\n",
    "Let's think about the general case. Consider an $N$-state Markov chain. The parameters of the Markov chain are the elements of the transition matrix, $P$.\n",
    "\n",
    "$$P \\equiv \\begin{bmatrix} p_{11} & p_{12} & \\dots & p_{1N} \\\\ p_{21} & \\vdots & \\ddots & \\vdots \\\\ p_{N1} & p_{N2} & \\dots & p_{NN} \\end{bmatrix}$$\n",
    "\n",
    "Let $\\{y_0, y_1, \\dots, y_T\\}$ be a sequence of observations generated from the $N$-state Markov chain, then our \"fitting\" procedure would assign the following value to $p_{ij}$:\n",
    "\n",
    "$$p_{ij} = \\frac{\\sum_{t=0}^T \\mathbb{1}_{y_{t} == i} \\mathbb{1}_{y_{t+1} == j}}{\\sum_{t=0}^T \\mathbb{1}_{y_{t} == i}}$$\n",
    "\n",
    "**Note**: If you'd like to understand why this procedure makes sense, we recommend computing $\\sum_{j=1}^N p_{ij}$ for a given $i$. What value do you get? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Counting frequencies**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def count_frequencies_individual(history):\n",
    "    \"\"\"\n",
    "    Computes the transition probabilities for a two-state\n",
    "    Markov chain\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : np.array(int, ndim=1)\n",
    "        An array with the state values of a two-state Markov chain\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        The probability of transitioning from state 0 to 1\n",
    "    beta : float\n",
    "        The probability of transitioning from state 1 to 0\n",
    "    \"\"\"\n",
    "    # Get length of the simulation and an index tracker\n",
    "    T = len(history)\n",
    "    idx = np.arange(T)\n",
    "\n",
    "    # Determine when the chain had values 0 and 1 -- Notice\n",
    "    # that we can't use the last value because we don't see\n",
    "    # where it transitions to\n",
    "    zero_idxs = idx[(history == 0) & (idx < T-1)]\n",
    "    one_idxs = idx[(history == 1) & (idx < T-1)]\n",
    "\n",
    "    # Check what percent of the t+1 values were 0/1\n",
    "    alpha = np.sum(history[zero_idxs+1]) / len(zero_idxs)\n",
    "    beta = np.sum(1 - history[one_idxs+1]) / len(one_idxs)\n",
    "\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Checking the fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def check_accuracy(T, alpha=0.25, beta=0.025):\n",
    "    \"\"\"\n",
    "    Checks the accuracy of our fit by printing the true values\n",
    "    and the fitted values for a given T\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : int\n",
    "        The length of our simulation\n",
    "    alpha : float\n",
    "        The probability that an individual goes from\n",
    "        unemployed to employed\n",
    "    beta : float\n",
    "        The probability that an individual goes from\n",
    "        employed to unemployed\n",
    "    \"\"\"\n",
    "    idx = np.arange(T)\n",
    "    alpha_np = np.ones(T)*alpha\n",
    "    beta_np = np.ones(T)*beta\n",
    "\n",
    "    # Simulate a sample history\n",
    "    emp_history = simulate_employment_history(alpha_np, beta_np, 0)\n",
    "\n",
    "    # Check the fit\n",
    "    alpha_hat, beta_hat = count_frequencies_individual(emp_history)\n",
    "    \n",
    "    print(f\"True alpha was {alpha} and fitted value was {alpha_hat}\")\n",
    "    print(f\"True beta was {beta} and fitted value was {beta_hat}\")\n",
    "    \n",
    "    return alpha, alpha_hat, beta, beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "check_accuracy(10_000, 0.25, 0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Well... If we observe 10,000 months of employment history for someone then we know that we can back out the parameters of our models...\n",
    "\n",
    "Unfortunately, our real world data won't have that much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What about for an entire lifetime of employment transitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "check_accuracy(45*12, 0.25, 0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What about for just two years of observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "check_accuracy(2*12, 0.25, 0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3 and 4 (second try)\n",
    "\n",
    "Data for the employment history of a single individual will not give us a good chance of fitting our model accurately...\n",
    "\n",
    "However, the BLS isn't infering EU/UE rates from its observation of a single individual. Rather, they're using a cross-section of individuals!\n",
    "\n",
    "Can we use a cross-section rather than for one individual?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, but, in order for a version of our \"frequency counting\" procedure to work, we want independence across individuals, i.e.\n",
    "\n",
    "$$\\text{Probability}(s_{i, t+1}, s_{j, t+1} | s_{i, t}, s_{j, t}, \\alpha, \\beta) = \\text{Probability}(s_{i, t+1} | s_{i, t}, \\alpha, \\beta) \\text{Probability}(s_{j, t+1} | s_{j, t}, \\alpha, \\beta)$$\n",
    "\n",
    "When we observed only a single individual, the Markov property did a lot of the work to get independence for us.\n",
    "\n",
    "When might this not be the case?\n",
    "\n",
    "* Change in government policy results in a \"jobs guarantee\"\n",
    "* Technological change results in the destruction of an entire industries jobs\n",
    "* Recession causes increased firing across entire country\n",
    "\n",
    "(Spoiler alert: Some of these will present problems for us... which is why we'll allow for $\\alpha$ and $\\beta$ to move each period)\n",
    "\n",
    "(TODO: Tom said he'd like to edit this cell again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simulating a cross-section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_employment_cross_section(alpha, beta, s_0, N=500):\n",
    "    \"\"\"\n",
    "    Simulates a cross-section of employment/unemployment using\n",
    "    the model we've described above.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : np.array(float, ndim=1)\n",
    "        The probability that an individual goes from\n",
    "        unemployed to employed\n",
    "    beta : np.array(float, ndim=1)\n",
    "        The probability that an individual goes from\n",
    "        employed to unemployed\n",
    "    s_0 : np.array(int, ndim=1)\n",
    "        The fraction of the population that begins in each\n",
    "        employment state\n",
    "    N : int\n",
    "        The number of individuals in our cross-section\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s_hist_cs : np.array(int, ndim=2)\n",
    "        An `N x T` matrix that contains an individual\n",
    "        history of employment along each row\n",
    "    \"\"\"\n",
    "    # Make sure transitions are same size and get the length\n",
    "    # of the simulation from the length of the transition\n",
    "    # probabilities\n",
    "    assert(len(alpha) == len(beta))\n",
    "    T = len(alpha)\n",
    "\n",
    "    # Check the fractions add to one and figure out how many\n",
    "    # zeros we should have\n",
    "    assert(np.abs(np.sum(s_0) - 1.0) < 1e-8)\n",
    "    Nz = np.floor(s_0[0]*N).astype(int)\n",
    "\n",
    "    # Allocate space to store the simulations\n",
    "    s_hist_cs = np.zeros((N, T+1), dtype=int)\n",
    "    s_hist_cs[Nz:, 0] = 1\n",
    "    \n",
    "    for i in range(N):\n",
    "        s_hist_cs[i, :] = simulate_employment_history(\n",
    "            alpha, beta, s_hist_cs[i, 0]\n",
    "        )\n",
    "    \n",
    "    return s_hist_cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha = np.ones(1)*0.25\n",
    "beta = np.ones(1)*0.025\n",
    "\n",
    "simulate_employment_cross_section(alpha, beta, np.array([0.35, 0.65]), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Store the simulation in pandas**\n",
    "\n",
    "Real world data will typically be stored in a DataFrame, so let's store our artificial data in a DataFrame as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def pandas_employment_cross_section(eu_ue_df, s_0, N=500):\n",
    "    \"\"\"\n",
    "    Simulate a cross-section of employment experiences\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eu_ue_df : pd.DataFrame\n",
    "        A DataFrame with columns `dt`, `alpha`, and `beta`\n",
    "        that have the monthly eu/ue transition rates\n",
    "    s_0 : np.array(float, ndim=1)\n",
    "        The fraction of the population that begins in each\n",
    "        employment state\n",
    "    N : int\n",
    "        The numbers of individuals in our cross-section\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        A DataFrame with the dates and an employment outcome\n",
    "        associated with each date of `eu_ue_df`\n",
    "    \"\"\"\n",
    "    # Make sure that `ue_ue_df` is sorted by date\n",
    "    eu_ue_df = eu_ue_df.sort_values(\"dt\")\n",
    "    alpha = eu_ue_df[\"alpha\"].to_numpy()\n",
    "    beta = eu_ue_df[\"beta\"].to_numpy()\n",
    "\n",
    "    # Simulate cross-section\n",
    "    employment_history = simulate_employment_cross_section(\n",
    "        alpha, beta, s_0, N\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(employment_history[:, :-1].T)\n",
    "    df = pd.concat([eu_ue_df[\"dt\"], df], axis=1)\n",
    "    df = pd.melt(\n",
    "        df, id_vars=[\"dt\"],\n",
    "        var_name=\"pid\", value_name=\"employment\"\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "T = 24\n",
    "eu_ue_df = pd.DataFrame(\n",
    "    {\n",
    "        \"dt\": pd.date_range(\"2018-01-01\", periods=T, freq=\"MS\"), \n",
    "        \"alpha\": np.ones(T)*0.25,\n",
    "        \"beta\": np.ones(T)*0.025\n",
    "    }\n",
    ")\n",
    "\n",
    "df = pandas_employment_cross_section(eu_ue_df, np.array([0.25, 0.75]), N=5_000)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simulating the CPS**\n",
    "\n",
    "Just to \"keep it interesting\", let's tie our hands in a similar way to how the BLS has their hands tied.\n",
    "\n",
    "We will simulate an individual's full employment history, but will only keep the subset that corresponds to when they would have been interviewed by the CPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cps_interviews(df, start_year, start_month):\n",
    "    \"\"\"\n",
    "    Takes an individual simulated employment/unemployment\n",
    "    history and \"interviews\" the individual as if they were\n",
    "    in the CPS\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A DataFrame with at least the columns `pid`, `dt`,\n",
    "        and `employment`\n",
    "    start_year : int\n",
    "        The year in which their interviewing begins\n",
    "    start_month : int\n",
    "        The month in which their interviewing begins\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cps : pd.DataFrame\n",
    "        A DataFrame with the same columns as `df` but only\n",
    "        with observations that correspond to the CPS\n",
    "        interview schedule for someone who starts\n",
    "        interviewing in f`{start_year}/{start_month}`\n",
    "    \"\"\"\n",
    "    # Get dates that are associated with being interviewed in\n",
    "    # the CPS\n",
    "    start_date_y1 = dt.datetime(start_year, start_month, 1)\n",
    "    dates_y1 = pd.date_range(start_date_y1, periods=4, freq=\"MS\")\n",
    "    start_date_y2 = dt.datetime(start_year+1, start_month, 1)\n",
    "    dates_y2 = pd.date_range(start_date_y2, periods=4, freq=\"MS\")\n",
    "    dates = dates_y1.append(dates_y2)\n",
    "\n",
    "    # Filter data that's not in the dates\n",
    "    cps = df.loc[df[\"dt\"].isin(dates), :]\n",
    "\n",
    "    return cps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interview = lambda x: cps_interviews(\n",
    "    x,\n",
    "    np.random.choice(x[\"dt\"].dt.year.unique()),\n",
    "    np.random.randint(1, 13)\n",
    ")\n",
    "\n",
    "cps_data = (\n",
    "    df.groupby(\"pid\")\n",
    "      .apply(\n",
    "          lambda x: interview(x)\n",
    "      )\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How many people are we observing per month?**\n",
    "\n",
    "If we think about the pattern used for the CPS interviews, we can form an idea of how many people might be interviewed each month.\n",
    "\n",
    "Consider if we started interviewing $m$ new individuals per month. How many would we be interviewing in any given month?\n",
    "\n",
    "Well. We'd at least be interviewing the $m$ new individuals. We would also be interviewing all of the individuals that had started their interviews in the previous 3 months. Additionally, we would be interviewing all of the individuals who had begun their interviews during those four months of the previous year.\n",
    "\n",
    "We can see this below -- Note that our \"survey\" begins in January 2018, so at first we only have $m$ individuals being interviewed, but, as the survey progresses, we move towards $8 m$ individuals being interviewed each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cps_data[\"dt\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Fitting to a cross-section**\n",
    "\n",
    "Well... Now our data look exactly like what the BLS uses to estimate the EU and UE transition rates from the raw data.\n",
    "\n",
    "In order to fit the data, we are going to continue using the \"frequency of transition\" concept that we previously proposed, but, we must account for the shape of the data we receive now.\n",
    "\n",
    "Let's think about the general case. Consider an $N$-state Markov chain. The parameters of the Markov chain are the elements of the transition matrix, $P$.\n",
    "\n",
    "$$P \\equiv \\begin{bmatrix} p_{11} & p_{12} & \\dots & p_{1N} \\\\ p_{21} & \\vdots & \\ddots & \\vdots \\\\ p_{N1} & p_{N2} & \\dots & p_{NN} \\end{bmatrix}$$\n",
    "\n",
    "Let $\\{ \\{y_{i, 0}, y_{i, 1}, \\dots, y_{i, T_i}\\} \\; \\forall i \\in \\{0, 1, \\dots, I\\}\\}$ be a $I$ sequences of observations generated from the $N$-state Markov chain, then our new \"fitting\" procedure would assign the following value to $p_{ij}$:\n",
    "\n",
    "$$p_{ij} = \\frac{\\sum_{m=0}^I \\sum_{t=0}^T \\mathbb{1}_{y_{m, t} == i} \\mathbb{1}_{y_{m, t+1} == j}}{\\sum_{m=0}^I \\sum_{t=0}^T \\mathbb{1}_{y_{m, t} == i}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cross-sectional counting frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cps_count_frequencies(df):\n",
    "    \"\"\"\n",
    "    Estimates the transition probability from employment\n",
    "    and unemployment histories of a CPS sample of\n",
    "    individuals\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A sample of individuals from the CPS survey. Must\n",
    "        have columns `dt`, `pid`, and `employment`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        The probability of transitioning from unemployment\n",
    "        to employment\n",
    "    beta : float\n",
    "        The probability of transitioning from employment\n",
    "        to unemployment\n",
    "    \"\"\"\n",
    "    # Set the index to be dt/pid\n",
    "    data_t = df.set_index([\"dt\", \"pid\"])\n",
    "\n",
    "    # Now find the \"t+1\" months and \"pid\"s\n",
    "    tp1 = data_t.index.get_level_values(\"dt\").shift(periods=1, freq=\"MS\")\n",
    "    pid = data_t.index.get_level_values(\"pid\")\n",
    "    idx = pd.MultiIndex.from_arrays([tp1, pid], names=[\"dt\", \"pid\"])\n",
    "\n",
    "    # Now \"index\" into the data and reset index\n",
    "    data_tp1 = (\n",
    "        data_t.reindex(idx)\n",
    "            .rename(columns={\"employment\": \"employment_tp1\"})\n",
    "    )\n",
    "    out = pd.concat(\n",
    "        [\n",
    "            data_t.reset_index().loc[:, [\"dt\", \"pid\", \"employment\"]],\n",
    "            data_tp1.reset_index()[\"employment_tp1\"]\n",
    "        ], axis=1, sort=True\n",
    "    ).dropna(subset=[\"employment_tp1\"])\n",
    "    out[\"employment_tp1\"] = out[\"employment_tp1\"].astype(int)\n",
    "\n",
    "    # Count how frequently we go from 0 to 1\n",
    "    out_zeros = out.query(\"employment == 0\")\n",
    "    alpha = out_zeros[\"employment_tp1\"].mean()\n",
    "    \n",
    "    # Count how frequently we go from 1 to 0\n",
    "    out_ones = out.query(\"employment == 1\")\n",
    "    beta = (1 - out_ones[\"employment_tp1\"]).mean()\n",
    "\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Checking accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def check_accuracy_cs(N, T, alpha=0.25, beta=0.025):\n",
    "    \"\"\"\n",
    "    Checks the accuracy of our fit by printing the true values\n",
    "    and the fitted values for a given T\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The total number of people we ever interview\n",
    "    T : int\n",
    "        The length of our simulation\n",
    "    alpha : float\n",
    "        The probability that an individual goes from\n",
    "        unemployed to employed\n",
    "    beta : float\n",
    "        The probability that an individual goes from\n",
    "        employed to unemployed\n",
    "    \"\"\"\n",
    "    alpha_beta_df = pd.DataFrame(\n",
    "        {\n",
    "            \"dt\": pd.date_range(\"2018-01-01\", periods=T, freq=\"MS\"), \n",
    "            \"alpha\": np.ones(T)*alpha,\n",
    "            \"beta\": np.ones(T)*beta\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Simulate the full cross-section\n",
    "    frac_unemployed = beta / (alpha + beta)\n",
    "    frac_employed = alpha / (alpha + beta)\n",
    "    df = pandas_employment_cross_section(\n",
    "        alpha_beta_df, np.array([frac_unemployed, frac_employed]), N\n",
    "    )\n",
    "\n",
    "    # Interview individuals according to the cps interviews\n",
    "    interview = lambda x: cps_interviews(\n",
    "        x,\n",
    "        np.random.choice(df[\"dt\"].dt.year.unique()),\n",
    "        np.random.randint(1, 13)\n",
    "    )\n",
    "    cps_data = (\n",
    "        df.groupby(\"pid\")\n",
    "          .apply(\n",
    "              lambda x: interview(x)\n",
    "          )\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Check the fit\n",
    "    alpha_hat, beta_hat = cps_count_frequencies(cps_data)\n",
    "    \n",
    "    print(f\"True alpha was {alpha} and fitted value was {alpha_hat}\")\n",
    "    print(f\"True beta was {beta} and fitted value was {beta_hat}\")\n",
    "    \n",
    "    return alpha, alpha_hat, beta, beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "check_accuracy_cs(1_000, 24, 0.25, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "check_accuracy_cs(500, 24, 0.25, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "check_accuracy_cs(100, 24, 0.25, 0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 7: Fit the model with actual CPS data\n",
    "\n",
    "We've downloaded (and cleaned!) a subset of real CPS data for the years 2018 and 2019.\n",
    "\n",
    "Let's see what our constant parameter model does with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load real CPS data that we've cleaned for you.\n",
    "cps_data = pd.read_parquet(\"cps_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What does this data contain?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cps_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Finding an employment history**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cps_count_sum = cps_data.groupby(\"pid\").agg(\n",
    "    {\"dt\": \"count\", \"employment\": \"sum\"}\n",
    ").sort_values(\"dt\")\n",
    "\n",
    "cps_count_sum.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cps_data.query(\"pid == 20180602828701\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Find an individual who experiences unemployment?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cps_count_sum.query(\"(dt == 8) & (employment < 8)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cps_data.query(\"pid == 20180307173302\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Computing $\\alpha$ and $\\beta$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha_cps, beta_cps = cps_count_frequencies(cps_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "beta_cps / (alpha_cps + beta_cps)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "06e05088bf2d2704501f452c5673235c92421ea24b381cad1d147a1ece3057ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('css': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
